--model=mlp_llama2_7b
--use_fromage=True
--n_layers=2
--hidden_dim=256
--final_dim=1
--finetune_type=lora
--dataset=txgnn_dcd
--n_epochs=30
--batch_size=24
--learning_rate=0.0003
--scheduler_type=cosine_decay_with_warmup
--valid_every=250
--weight_decay=0.01
--wandb_track=True
