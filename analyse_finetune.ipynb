{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnn3Z24OgzcO3TBXWAzYoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonathan-schwarz/TxGNNv2/blob/main/analyse_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install -q accelerate\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q chardet\n",
        "!pip install -q goatools\n",
        "!pip install -q gpytorch\n",
        "!pip install -q matplotlib\n",
        "!pip install -q numpy\n",
        "!pip install -q peft\n",
        "!pip install -q seaborn\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q tqdm\n",
        "!pip install -q transformers[torch]\n",
        "!pip install -q trl\n",
        "!pip install -q wandb"
      ],
      "metadata": {
        "id": "ErnNHG_Csicy",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports & Setup\n",
        "\n",
        "import collections\n",
        "import gpytorch\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "from gpytorch.models import ApproximateGP\n",
        "from gpytorch.variational import CholeskyVariationalDistribution\n",
        "from gpytorch.variational import GridInterpolationVariationalStrategy, UnwhitenedVariationalStrategy\n",
        "from scipy.stats import entropy\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        "    Dataset,\n",
        "    DatasetDict)\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    pipeline,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PromptEncoderConfig,\n",
        ")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "DATA_PATH = '/content/gdrive/My Drive/Work/Harvard/TxGNN/Data'"
      ],
      "metadata": {
        "id": "_aSwnsBgvtKp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data\n",
        "\n",
        "\n",
        "DD_ETYPES = {\n",
        "    'did' : ('drug', 'indication', 'disease'),\n",
        "    'dod' : ('drug', 'off-label use', 'disease'),\n",
        "    'dcd' : ('drug', 'contraindication', 'disease'),\n",
        "    'drid': ('disease', 'rev_indication', 'drug'),\n",
        "    'drod': ('disease', 'rev_off-label use', 'drug'),\n",
        "    'drcd': ('disease', 'rev_contraindication', 'drug'),\n",
        "}\n",
        "\n",
        "\n",
        "def load_split(dataset, mode, merge_str):\n",
        "    u_names = dataset[mode + '_u_names']\n",
        "    v_names = dataset[mode + '_v_names']\n",
        "    text = np.concatenate([u_names, v_names], axis=1)\n",
        "    labels = dataset[mode + '_labels']\n",
        "\n",
        "    text = [merge_str.format(text[i][0], text[i][1]) for i in range(text.shape[0])]\n",
        "    data_dict = {\n",
        "        'text': text,\n",
        "        'label': labels.astype(np.int32)[:, 0].tolist(),\n",
        "    }\n",
        "    dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def load_txgnn_dataset_for_llm(edge_type, dataset):\n",
        "    if 'did' == edge_type:\n",
        "        merge_str = 'Is {} an effective treatment for {}?'\n",
        "    elif 'dod' == edge_type:\n",
        "        merge_str = 'Is {} effective for off-label use on {}?'\n",
        "    elif 'dcd' == edge_type:\n",
        "        merge_str = 'Should {} be avoided for patients suffering from {}?'\n",
        "    else:\n",
        "        assert False, 'Reverse cases not yet used'\n",
        "\n",
        "    e_type = DD_ETYPES[edge_type]\n",
        "\n",
        "    train_dataset = load_split(dataset, 'train', merge_str)\n",
        "    valid_dataset = load_split(dataset, 'valid', merge_str)\n",
        "    test_dataset = load_split(dataset, 'test', merge_str)\n",
        "    full_dataset = DatasetDict({'train': train_dataset, 'valid': valid_dataset, 'test': test_dataset})\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "device =  torch.device('cuda:0') if torch.cuda.is_available() else torch.device( \"cpu\")\n",
        "\n",
        "dataset = 'did'\n",
        "DD_TYPES = {\n",
        "    'did': 'drug_indication_disease',\n",
        "    'dod': 'drug_off-label use_disease',\n",
        "    'dcd': 'drug_contraindication_disease',\n",
        "    'drid': 'disease_rev_indication_drug',\n",
        "    'drod': 'disease_rev_off-label use_drug',\n",
        "    'drcd': 'disease_rev_contraindication_drug',\n",
        "}\n",
        "\n",
        "data = np.load(os.path.join(DATA_PATH, '{}.npz').format(DD_TYPES[dataset]))\n",
        "\n",
        "# For language models\n",
        "txgnn_llm = load_txgnn_dataset_for_llm(dataset, data)\n",
        "\n",
        "# For everything else\n",
        "train_x = torch.Tensor(np.concatenate(\n",
        "    [data['train_h_u'], data['train_h_v']], axis=1)).to(device)\n",
        "train_y = torch.Tensor(data['train_labels']).to(device)\n",
        "train_names = np.concatenate(\n",
        "    [data['train_u_names'], data['train_v_names']], axis=1)\n",
        "train_set = torch.utils.data.TensorDataset(train_x, train_y)\n",
        "\n",
        "valid_x = torch.Tensor(np.concatenate(\n",
        "    [data['valid_h_u'], data['valid_h_v']], axis=1)).to(device)\n",
        "valid_y = torch.Tensor(data['valid_labels']).to(device)\n",
        "valid_names = np.concatenate(\n",
        "    [data['valid_u_names'], data['valid_v_names']], axis=1)\n",
        "valid_set = torch.utils.data.TensorDataset(valid_x, valid_y)\n",
        "\n",
        "test_x = torch.Tensor(np.concatenate(\n",
        "    [data['test_h_u'], data['test_h_v']], axis=1)).to(device)\n",
        "test_y = torch.Tensor(data['test_labels']).to(device)\n",
        "test_names = np.concatenate(\n",
        "    [data['test_u_names'], data['test_v_names']], axis=1)\n",
        "test_set = torch.utils.data.TensorDataset(test_x, test_y)\n",
        "\n",
        "data_dim = train_x.shape[1]\n",
        "num_classes = 2\n",
        "num_valid_points = valid_x.shape[0]\n",
        "num_test_points = test_x.shape[0]\n",
        "\n",
        "train_x_ = train_x.detach().cpu().numpy()\n",
        "train_y_ = train_y.detach().cpu().numpy()\n",
        "valid_x_ = valid_x.detach().cpu().numpy()\n",
        "valid_y_ = valid_y.detach().cpu().numpy()\n",
        "test_x_ = test_x.detach().cpu().numpy()\n",
        "test_y_ = test_y.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "t5Scforfx4C3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT9YSmjtsTDL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Models\n",
        "\n",
        "class FeatureExtractor(torch.nn.Sequential):\n",
        "    def __init__(self, data_dim, n_layers=3, hidden_dim=100, final_dim=2):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        if n_layers == 1:\n",
        "            self.add_module('linear1', torch.nn.Linear(data_dim, final_dim))\n",
        "        else:\n",
        "            self.add_module('linear1', torch.nn.Linear(data_dim, hidden_dim))\n",
        "            self.add_module('relu1', torch.nn.ReLU())\n",
        "            for i in range(1, n_layers):\n",
        "                self.add_module('linear{}'.format(i+1), torch.nn.Linear(hidden_dim, hidden_dim))\n",
        "                self.add_module('relu{}'.format(i+1), torch.nn.ReLU())\n",
        "            self.add_module('linear4', torch.nn.Linear(hidden_dim, final_dim))\n",
        "\n",
        "class GPClassificationModel(ApproximateGP):\n",
        "    def __init__(self, dataset, num_dim, strategy,\n",
        "                 grid_bounds, grid_size=64, inducing_x=None):\n",
        "        if 'toy' == dataset:\n",
        "            variational_distribution = CholeskyVariationalDistribution(\n",
        "                num_inducing_points=inducing_x.size(0))\n",
        "            variational_strategy = UnwhitenedVariationalStrategy(\n",
        "                self, inducing_points=inducing_x,\n",
        "                variational_distribution=variational_distribution,\n",
        "                learn_inducing_locations=False\n",
        "            )\n",
        "            super(GPClassificationModel, self).__init__(variational_strategy)\n",
        "\n",
        "            # if self.feature_extractor is None:\n",
        "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "            #else:\n",
        "            #    self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
        "            #        gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
        "            #        num_dims=2, grid_size=100\n",
        "            #    )\n",
        "        elif False:#'txgnn_d' in dataset:\n",
        "            variational_distribution = CholeskyVariationalDistribution(\n",
        "                num_inducing_points=grid_size, batch_shape=torch.Size([num_dim])\n",
        "            )\n",
        "            variational_strategy = GridInterpolationVariationalStrategy(\n",
        "                    self, grid_size=grid_size, grid_bounds=[grid_bounds],\n",
        "                    variational_distribution=variational_distribution,\n",
        "            )\n",
        "            super(GPClassificationModel, self).__init__(variational_strategy)\n",
        "\n",
        "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
        "                gpytorch.kernels.RBFKernel(\n",
        "                    lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
        "                        math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html\n",
        "            variational_distribution = CholeskyVariationalDistribution(\n",
        "                num_inducing_points=grid_size, batch_shape=torch.Size([num_dim])\n",
        "            )\n",
        "            if 'grid_interpolation' == strategy:\n",
        "                variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
        "                    GridInterpolationVariationalStrategy(\n",
        "                        self, grid_size=grid_size, grid_bounds=[grid_bounds],\n",
        "                        variational_distribution=variational_distribution,\n",
        "                    ), num_tasks=num_dim,\n",
        "                )\n",
        "            elif 'unwhitened' == strategy:\n",
        "                # TODO(schwarzjn): This needs to be tested\n",
        "                assert inducing_x is not None\n",
        "                variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
        "                    UnwhitenedVariationalStrategy(\n",
        "                        self, inducing_points=inducing_x,\n",
        "                        variational_distribution=variational_distribution,\n",
        "                        learn_inducing_locations=False\n",
        "                    ), num_tasks=num_dim,\n",
        "                )\n",
        "            super(GPClassificationModel, self).__init__(variational_strategy)\n",
        "\n",
        "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
        "                gpytorch.kernels.RBFKernel(\n",
        "                    lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(\n",
        "                        math.exp(-1), math.exp(1), sigma=0.1, transform=torch.exp\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "        return latent_pred\n",
        "\n",
        "\n",
        "class DKLModel(gpytorch.Module):\n",
        "    def __init__(self, dataset, feature_extractor, strategy, num_dim, grid_bounds=(-10., 10.),\n",
        "                 inducing_x=None):\n",
        "        super(DKLModel, self).__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.gp_layer = GPClassificationModel(\n",
        "            dataset=dataset, strategy=strategy,\n",
        "            num_dim=num_dim, grid_bounds=grid_bounds,\n",
        "            inducing_x=inducing_x)\n",
        "        self.grid_bounds = grid_bounds\n",
        "        self.num_dim = num_dim\n",
        "        self.dataset = dataset\n",
        "\n",
        "        # This module will scale the NN features so that they're nice values\n",
        "        self.scale_to_bounds = gpytorch.utils.grid.ScaleToBounds(self.grid_bounds[0], self.grid_bounds[1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        features = self.scale_to_bounds(features)\n",
        "        # This next line makes it so that we learn a GP for each feature\n",
        "        features = features.transpose(-1, -2).unsqueeze(-1)\n",
        "        res = self.gp_layer(features)\n",
        "        return res\n",
        "\n",
        "def _preprocess_function(examples):\n",
        "    max_length = 20  # Length of bert\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-hf', trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "tokenized_dataset = txgnn_llm.map(_preprocess_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "task_type = \"SEQ_CLS\"\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "use_bf16 = False\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "bf16 = False\n",
        "use_4bit = True\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerating training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "        bf16 = True\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = 'dkl'  # @param ['distmult', 'dkl', 'mlp', 'llama2_7b']\n",
        "best_metric = 'loss'  # @param ['acc', 'auroc_auprc', 'loss']\n",
        "run = 'schwarzjn/TxGNNv2/crtlxrth' #@param\n",
        "filename = 'model_ckpt_2024-01-24 16:04:08.986185' #@param\n",
        "\n",
        "likelihood = None\n",
        "if 'dkl' == model_type:\n",
        "  # Initialize model and likelihood\n",
        "  feature_extractor = FeatureExtractor(\n",
        "    data_dim=data_dim,\n",
        "    hidden_dim=256,\n",
        "    n_layers=3,\n",
        "    final_dim=256).to(device)\n",
        "  model = DKLModel('txgnn_' + dataset, feature_extractor, strategy='grid_interpolation',\n",
        "                    num_dim=256, inducing_x=None).to(device)\n",
        "  # TODO(schwarzjn): Use BernoulliLikelihood\n",
        "  #if num_classes == 2:\n",
        "  #    likelihood = gpytorch.likelihoods.BernoulliLikelihood().to(device)\n",
        "  #else:\n",
        "  likelihood = gpytorch.likelihoods.SoftmaxLikelihood(\n",
        "          num_features=model.num_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "  best_model = wandb.restore('checkpoints/finetune/txgnn_did_finetune_{}/{}/best_{}_model.pt'.format(model_type, filename, best_metric), run_path=run)\n",
        "\n",
        "  # use the \"name\" attribute of the returned object if your framework expects a filename, e.g. as in Keras\n",
        "  ckpt = torch.load(best_model.name, map_location=device)\n",
        "  model.load_state_dict(ckpt['model_state_dict'])\n",
        "  likelihood.load_state_dict(ckpt['likelihood_state_dict'])\n",
        "elif 'mlp' == model_type:\n",
        "  model = FeatureExtractor(\n",
        "      data_dim=data_dim,\n",
        "      hidden_dim=256,\n",
        "      n_layers=3,\n",
        "      final_dim=num_classes if num_classes > 2 else 1).to(device)\n",
        "\n",
        "  best_model = wandb.restore('checkpoints/finetune/txgnn_did_finetune_{}/{}/best_{}_model.pt'.format(model_type, filename, best_metric), run_path=run)\n",
        "\n",
        "  # use the \"name\" attribute of the returned object if your framework expects a filename, e.g. as in Keras\n",
        "  ckpt = torch.load(best_model.name, map_location=device)\n",
        "  model.load_state_dict(ckpt['model_state_dict'])\n",
        "elif 'llama2' in model_type:\n",
        "  id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "  label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'NousResearch/Llama-2-7b-hf' if '7b' in model_type else 'NousResearch/Llama-2-13b-hf',\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map,\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "  )\n",
        "  peft_config = LoraConfig('/content/checkpoints/finetune_llm/txgnn_{}_finetune_{}/{}/{}_txgnn_did/'.format(dataset, model_type, filename, model_type))\n",
        "  model.add_adapter(peft_config)\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='~/logs',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    fp16=False,\n",
        "    bf16=use_bf16,\n",
        "  )\n",
        "  trainer = Trainer(\n",
        "      args=training_args,\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      data_collator=data_collator,\n",
        "  )"
      ],
      "metadata": {
        "id": "zzkrToyksgZ0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "with torch.no_grad(), gpytorch.settings.num_likelihood_samples(64):\n",
        "  if 'dkl' == model_type:\n",
        "    output = model(valid_x)\n",
        "    full_pred_prob = likelihood(output).probs[:, :, 1:]\n",
        "    pred_prob = full_pred_prob.mean(0)\n",
        "    pred_label = pred_prob.ge(0.5).float()\n",
        "  elif 'llama' in model_type:\n",
        "    # llm_predictions = trainer.predict(tokenized_dataset['valid'])\n",
        "    pred_prob = torch.nn.Softmax()(torch.Tensor(llm_predictions.predictions).to(device))[:, :1]\n",
        "    pred_label = pred_prob.ge(0.5).float()\n",
        "  else:\n",
        "    output = model(valid_x)\n",
        "    pred_prob = torch.sigmoid(output)\n",
        "    pred_label = pred_prob.ge(0.5).float()\n",
        "\n",
        "pred_prob_ = pred_prob.detach().cpu().numpy()\n",
        "pred_label_ = pred_label.detach().cpu().numpy()\n",
        "\n",
        "# Probability of predicting the correct label\n",
        "pred_prob_both = np.concatenate([1-pred_prob_, pred_prob_], axis=1)\n",
        "rel_pred_prob_ = pred_prob_both[\n",
        "    range(valid_y_.shape[0]), valid_y_.astype(np.int32).flatten()][:, np.newaxis]\n",
        "\n",
        "\n",
        "pred_prob_correct_ = pred_prob_[pred_label_ == valid_y_][:, np.newaxis]\n",
        "pred_prob_incorrect_ = pred_prob_[pred_label_ != valid_y_][:, np.newaxis]\n",
        "valid_auroc = roc_auc_score(valid_y_, pred_prob_)\n",
        "valid_auprc = average_precision_score(valid_y_, pred_prob_)\n",
        "valid_acc = (pred_label.eq(valid_y.view_as(pred_label)).cpu().numpy().sum()) / valid_y_.shape[0]\n",
        "valid_entropy = entropy(np.concatenate([pred_prob_, 1 - pred_prob_], axis=1), axis=1).sum()\n",
        "valid_entropy_correct = entropy(np.concatenate([pred_prob_correct_, 1 - pred_prob_correct_], axis=1), axis=1).sum()\n",
        "valid_entropy_incorrect = entropy(np.concatenate([pred_prob_incorrect_, 1 - pred_prob_incorrect_], axis=1), axis=1).sum()\n",
        "\n",
        "print('AUROC: {:.3f} AUPRC: {:.3f} Acc: {:.3f} Total Entropy: {:.3f} Correct Entropy: {:.3f} Incorrect Entropy: {:.3f}'.format(\n",
        "    valid_auroc, valid_auprc, valid_acc, valid_entropy, valid_entropy_correct, valid_entropy_incorrect\n",
        "))\n",
        "\n",
        "fpr, tpr, _ = roc_curve(valid_y_, pred_prob_)"
      ],
      "metadata": {
        "id": "LPhqt_EStSEL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "sns.set_style('white')\n",
        "ax = plt.gca()\n",
        "\n",
        "bins = np.linspace(0, 1, 11)\n",
        "bincenters = 0.5*(bins[1:]+bins[:-1])\n",
        "counts_in_bin, bin_edges = np.histogram(rel_pred_prob_, bins=bins)\n",
        "\n",
        "# Assign to probabiltiy bins by mean prediction\n",
        "bin_in_mat = np.digitize(rel_pred_prob_, bins, right=False)\n",
        "\n",
        "# Get the mean standard deviation for each item in each bin\n",
        "# (full_pred_prob.max(axis=0).values - full_pred_prob.min(axis=0).values).shape\n",
        "yerr = [full_pred_prob.std(axis=0)[bin_in_mat == i].mean().cpu().numpy() for i in range(1, 11)]\n",
        "\n",
        "ax.bar(bincenters, counts_in_bin, width=0.05, yerr=yerr)\n",
        "ax.set_xticks(bins[:-1])\n",
        "#ax.set_yticks([])\n",
        "ax.set_title('Probability of predicing the correct label')\n",
        "\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "KKUzlrdKO1m3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Uncertainty over the entire distribution\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.plot(yerr)"
      ],
      "metadata": {
        "id": "gPEllG2UKt4Q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Overall distribution of predictions\n",
        "\n",
        "sns.set_style('white')\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "bins = np.linspace(0, 1, 11)\n",
        "ax.set_xticks(bins[:-1])\n",
        "ax.set_yticks([])\n",
        "ax.set_title('Overall prediction distribution')\n",
        "ax.hist(pred_prob_, bins=bins)\n",
        "\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "yCFKV2Fr9wxR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FP/TP Confusion Matrix\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(valid_y_, pred_label_).ravel()\n",
        "\n",
        "print('True pos: {} False pos: {} True neg: {} False neg: {}'.format(tp, fp, tn, fn))\n",
        "# 86% of the items we predicted to be positve are positive, we found 69% of positve examples\n",
        "print('Precision: {} Recall: {}'.format(tp/(tp+fp), tp/(tp+fn)))"
      ],
      "metadata": {
        "id": "1XDJOOsP6cEI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = plt.gca()\n",
        "\n",
        "bins = np.linspace(0, 1, 11)\n",
        "\n",
        "#n_c, _, _ = ax.hist(pred_prob_correct_, color='g', alpha=0.5)\n",
        "n_ic, _, _ = ax.hist(pred_prob_incorrect_, color='r', alpha=0.5)\n",
        "\n",
        "norm_bins_c = (n_c / pred_prob_correct_.shape[0]) + 0.1  # Smoothing\n",
        "entropy_c = - (norm_bins_c * np.log(norm_bins_c)).sum()\n",
        "print('Entropy of correct predictions: {}'.format(entropy_c))\n",
        "\n",
        "norm_bins_ic = (n_ic / pred_prob_incorrect_.shape[0]) + 0.1  # Smoothing\n",
        "entropy_ic = - (norm_bins_ic * np.log(norm_bins_ic)).sum()\n",
        "print('Entropy of incorrect predictions: {}'.format(entropy_ic))\n",
        "\n",
        "#_ = plt.hist(pred_prob_, bins=bins)"
      ],
      "metadata": {
        "id": "MD4cjWHz5lzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('white')\n",
        "ax = plt.gca()\n",
        "ax.plot(bins[:-1], norm_bins_c, marker='o', c='g', label='Correct predictions')\n",
        "ax.plot(bins[:-1], norm_bins_ic, marker='o', c='r', label='Incorrect predictions')\n",
        "\n",
        "ax.set_xticks(bins[::2])\n",
        "\n",
        "ax.legend(frameon=False)\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "gTLtCI_tAyZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pca = PCA(n_components=2)\n",
        "#x_proj_pca = pca.fit_transform(test_x)\n",
        "valid_x_ = valid_x.detach().cpu().numpy()\n",
        "x_proj_tsne = TSNE(n_components=2, perplexity=30).fit_transform(valid_x_)\n",
        "x_proj_tsne_drugs = TSNE(n_components=2, perplexity=30).fit_transform(valid_x_[:, :512])\n",
        "x_proj_tsne_diseases = TSNE(n_components=2, perplexity=30).fit_transform(valid_x_[:, 512:])"
      ],
      "metadata": {
        "id": "750GnMRZG451"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mode = 'both'  # @param ['both', 'diseases', 'drugs']\n",
        "\n",
        "\n",
        "if mode == 'diseases':\n",
        "  x = x_proj_tsne_diseases\n",
        "elif mode == 'drugs':\n",
        "  x = x_proj_tsne_drugs\n",
        "else:\n",
        "  x = x_proj_tsne\n",
        "\n",
        "how_many = 20\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'tsne_x': x[:, 0],\n",
        "    'tsne_y': x[:, 1],\n",
        "    'drug': valid_names[:, 0],\n",
        "    'drug_counter': np.zeros_like(valid_names[:, 0]),\n",
        "    'disease': valid_names[:, 1],\n",
        "    'disease_counter': np.zeros_like(valid_names[:, 0]),\n",
        "})\n",
        "\n",
        "unique_diseases = np.unique(valid_names[:, 1])\n",
        "disease_counter = collections.Counter(valid_names[:, 1])\n",
        "most_common_diseases, count_diseases = zip(*disease_counter.most_common()[:how_many])\n",
        "\n",
        "unique_drugs = np.unique(valid_names[:, 0])\n",
        "drug_counter = collections.Counter(valid_names[:, 0])\n",
        "most_common_drugs, count_drugs = zip(*drug_counter.most_common()[:how_many])\n",
        "\n",
        "print('{} unique drugs, {} unqiue diseases'.format(len(unique_drugs), len(unique_diseases)))\n",
        "\n",
        "for k, v in disease_counter.items():\n",
        "  df.loc[df['disease'] == k, 'disease_counter'] = v\n",
        "\n",
        "for k, v in drug_counter.items():\n",
        "  df.loc[df['drug'] == k, 'drug_counter'] = v\n",
        "\n",
        "# Get the mean\n",
        "df_centres = df.groupby('disease').mean(['tsne_x', 'tsne_y'])"
      ],
      "metadata": {
        "id": "NQRJvdO8NwiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "sd8C1N2FQ2o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import colorcet as cc\n",
        "\n",
        "blobs, labels = make_blobs(n_samples=1000, centers=len(unique_diseases), center_box=(-100, 100))\n",
        "custom_palette = sns.color_palette(cc.glasbey, n_colors=len(unique_diseases))\n",
        "diseases_to_colors = dict(zip(unique_diseases.tolist(), custom_palette))\n",
        "\n",
        "sns.palplot(custom_palette)\n",
        "\n",
        "blobs, labels = make_blobs(n_samples=1000, centers=len(unique_drugs), center_box=(-100, 100))\n",
        "custom_palette = sns.color_palette(cc.glasbey, n_colors=len(unique_drugs))\n",
        "drugs_to_colors = dict(zip(unique_drugs.tolist(), custom_palette))"
      ],
      "metadata": {
        "id": "Qn-7Ct79WdO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('white')\n",
        "\n",
        "from matplotlib.lines import Line2D\n",
        "df_centres = df.groupby('disease').mean(['tsne_x', 'tsne_y'])\n",
        "\n",
        "with_names = False # @param {type:\"boolean\"}\n",
        "\n",
        "colors = [diseases_to_colors[valid_names[i, 1]] for i in range(x_proj_tsne.shape[0])]\n",
        "markers = list(Line2D.markers.keys())\n",
        "\n",
        "#colors = sns.color_palette(\"Paired\", 10)\n",
        "\n",
        "_, ax = plt.subplots(figsize=(1.33*10, 10))\n",
        "ax.scatter(x[:, 0], x[:, 1], c=colors, marker='.', alpha=0.5)\n",
        "\n",
        "ax.set_xlim([-50, 50])\n",
        "\n",
        "if with_names:\n",
        "  already_used = []\n",
        "  for i in range(x.shape[0]):\n",
        "    name = valid_names[i, 1]\n",
        "    if name in most_common_diseases:\n",
        "      if name not in already_used:\n",
        "        print(name)\n",
        "        x_adj, y_adj = 2, 2\n",
        "        xy = (df_centres['tsne_x'][name]+x_adj, df_centres['tsne_y'][name]+y_adj)\n",
        "        ax.annotate(name, xy=xy, fontsize=7, c=colors[i], weight='bold')\n",
        "        ax.scatter(xy[0], xy[1], c=colors[i], marker='.', label=name)\n",
        "        already_used.append(valid_names[i, 1])\n",
        "        #print('{}, {}'.format(test_names[i, 1], xy))\n",
        "\n",
        "ax.set_title('TxGNN Validation Set embeddings')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "#ax.legend(ncols=2, frameon=False)\n",
        "ax.set_xlim([-55, 55])\n",
        "sns.despine(left=True, bottom=True)"
      ],
      "metadata": {
        "id": "F4ZC_qrdLyV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('white')\n",
        "\n",
        "_, ax = plt.subplots(figsize=(1.33*10, 10))\n",
        "colors = ['g' if pred_label_[i] == valid_y_[i] else 'r' for i in range(x_proj_tsne.shape[0])]\n",
        "ax.scatter(x_proj_tsne[:, 0], x_proj_tsne[:, 1], c=colors, marker='.', alpha=0.5)\n",
        "\n",
        "#ax.legend()\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "sns.despine()\n",
        "\n",
        "\n",
        "ax.set_title('TxGNN Validation Set predictions')\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "#ax.legend(ncols=2, frameon=False)\n",
        "ax.set_xlim([-55, 55])\n",
        "sns.despine(left=True, bottom=True)"
      ],
      "metadata": {
        "id": "ZgAi-I60X7cv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}