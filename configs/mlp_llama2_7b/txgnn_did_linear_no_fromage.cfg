--batch_size=24
--dataset=txgnn_did
--eval_batch_size=128
--final_dim=1
--finetune_type=lora
--hidden_dim=256
--learning_rate=0.0003
--model=mlp_llama2_7b
--n_epochs=10
--n_layers=1
--scheduler_type=cosine_decay_with_warmup
--use_fromage=False
--valid_every=250
--wandb_track=True
--weight_decay=0.01
